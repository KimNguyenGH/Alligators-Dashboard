---
title: "Analyzing COVID-19 Tweets from Mar 2020 & 2021"
author: "A. Plumber, E. Lin, K. Nguyen, R. Karbowicz, M. Aines"
output: flexdashboard::flex_dashboard
---

```{r setup, include=FALSE}
library(flexdashboard)
library(plotly)
library(htmltools)
library(rmarkdown)
library(tidyverse)
library(scales)
library(tidytext)
library(wordcloud)
library(textdata)

library(ranger)
library(caret)
library(vip)

library(kableExtra)

library(tm)
library(topicmodels)
library(reshape2)
library(ggplot2)
library(pals)

library(quanteda)
library(seededlda)
library(newsmap)
library(maps)
library(LSX)
library(quanteda.textstats)
library(quanteda.textplots)
library(SnowballC)
library(gt)

library(sjPlot)
library(sjmisc)
library(sjlabelled)

twenty <- 
  read_csv('03-30-2020.csv', 
           col_types = cols(id = col_character())) %>% 
  select(id, text = full_text, year)

twentyone <- 
  read_csv('03-30-2021.csv', 
           col_types = cols(id = col_character())) %>% 
  select(id, text = full_text, year)

reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"

twenty_words <- 
  twenty %>%
  filter(!str_detect(text, '^"')) %>%
  mutate(text = str_replace_all(text, 
                                "https://t.co/[A-Za-z\\d]+|&amp;", 
                                "")) %>%
  unnest_tokens(word, text, 
                token = "regex", 
                pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))

twentyone_words <- 
  twentyone %>%
  filter(!str_detect(text, '^"')) %>%
  mutate(text = str_replace_all(text, 
                                "https://t.co/[A-Za-z\\d]+|&amp;", 
                                "")) %>%
  unnest_tokens(word, text, 
                token = "regex", 
                pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))

twenty_most_common <- 
  twenty_words %>%
  count(word, sort = TRUE) %>%
  head(20) %>%
  mutate(word = reorder(word, n))

graph1 <-
  ggplotly(
  twenty_most_common %>%
  ggplot(aes(x = word, y = n)) +
  geom_bar(stat = "identity") +
  ylab("Occurrences") +
  coord_flip() + 
  ggtitle("2020 COVID-19 Tweets Word Frequency")
  )

twentyone_most_common <- 
  twentyone_words %>%
  count(word, sort = TRUE) %>%
  head(20) %>%
  mutate(word = reorder(word, n))

graph2 <- 
  ggplotly(
  twentyone_most_common %>%
  ggplot(aes(x = word, y = n)) +
  geom_bar(stat = "identity") +
  ylab("Occurrences") +
  coord_flip() +
  ggtitle("2021 COVID-19 Tweets Word Frequency")
  )

#added - common words by proportion

compare_most_common <- full_join(twenty_most_common, twentyone_most_common, by = "word")

colnames(compare_most_common) <- c("Word", "2020", "2021")

# most common words by proportion 2020
compare_most_common %>%  mutate(`2020 proportion` = compare_most_common$`2020`/4666) -> prp_twenty_most_common

prp_twenty_most_common[!is.na(prp_twenty_most_common$`2020 proportion`),] -> prp_twenty_most_common

prp_twenty_most_common %>%  select("Word",`2020 proportion`) %>% arrange(-`2020 proportion`) -> prp_twenty_most_common

# most common words by proportion 2021

compare_most_common %>%  mutate(`2021 proportion` = compare_most_common$`2021`/5341) -> prp_twentyone_most_common

prp_twentyone_most_common[!is.na(prp_twentyone_most_common$`2021 proportion`),] -> prp_twentyone_most_common

prp_twentyone_most_common %>%  select("Word",`2021 proportion`) %>% arrange(-`2021 proportion`) -> prp_twentyone_most_common

#graph

prpgraph_twenty_most_common <-
  ggplotly( 
  prp_twenty_most_common %>%
  ggplot(aes(x = Word, y = `2020 proportion`)) +
  geom_bar(stat = "identity" , fill="#1DA1F2") +
  ylab("Frequency") +
  coord_flip() + 
  ggtitle("2020 COVID-19 Tweets Word Frequency By Proportion")
  )

prpgraph_twentyone_most_common <-
  ggplotly( 
  prp_twentyone_most_common %>%
  ggplot(aes(x = Word, y = `2021 proportion`)) +
  geom_bar(stat = "identity", fill="#1DA1F2") +
  ylab("Frequency") +
  coord_flip() + 
  ggtitle("2021 COVID-19 Tweets Word Frequency By Proportion")
  )
  
## 
twenty_cloud <- 
  twenty_words  %>% 
  count(word) %>% 
  arrange(-n)

twentyone_cloud <- 
  twentyone_words  %>% 
  count(word) %>% 
  arrange(-n)

twenty_wordcounts <- 
  twenty %>%
  mutate(tweetLength = str_length(text)) %>% 
  filter(tweetLength < 500)

twentyone_wordcounts <- 
  twentyone %>%
  mutate(tweetLength = str_length(text)) %>% 
  filter(tweetLength < 500)

nrc <- read_rds("nrc.rds")

twenty_sentiment <-
  inner_join(twenty_words, nrc, by = "word")

twentyone_sentiment <-
  inner_join(twentyone_words, nrc, by = "word")

twenty_sentiment_analysis <- 
  twenty_sentiment %>% 
  count(word, sentiment) %>% 
  group_by(sentiment)

graph3 <- 
  ggplotly(
  twenty_sentiment_analysis %>%  
  top_n(15) %>% 
  ggplot(aes(x = sentiment, y = n )) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ylab("Frequency") +
  xlab("Sentiment") +
  labs(title="30 Mar 2020 Sentiments")
  )

twentyone_sentiment_analysis <- 
  twentyone_sentiment %>% 
  count(word, sentiment) %>% 
  group_by(sentiment)

graph4 <-
  ggplotly(
  twentyone_sentiment_analysis %>%  
  top_n(15) %>% 
  ggplot(aes(x = sentiment, y = n )) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ylab("Frequency") +
  xlab("Sentiment") +
  labs(title="30 Mar 2021 Sentiments")
  )

twenty_sentiment_analysis %>% filter(!sentiment %in% c("positive", "negative")) %>% 
  mutate(sentiment = reorder(sentiment, -n),
         word = reorder(word, -n)) %>% top_n(10) -> twenty_sentiment_analysis2

graph5 <-
  ggplotly(
  ggplot(twenty_sentiment_analysis2, aes(x=word, y=n, fill = n)) +
  facet_wrap(~ sentiment, scales = "free")+ 
  geom_bar(stat ="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  labs(y="count", title="30 Mar 2020 Sentiment Words")
  )

twentyone_sentiment_analysis %>% filter(!sentiment %in% c("positive", "negative")) %>% 
  mutate(sentiment = reorder(sentiment, -n),
         word = reorder(word, -n)) %>% top_n(10) -> twentyone_sentiment_analysis2

graph6 <-
  ggplotly(
  ggplot(twentyone_sentiment_analysis2, aes(x=word, y=n, fill = n)) +
  facet_wrap(~ sentiment, scales = "free")+ 
  geom_bar(stat ="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  labs(y="count", title="30 Mar 2021 Sentiment Words")
  )

afinn <- read_csv('afinn.csv')

twenty_afinn <-    
 inner_join(twenty_words, 
            afinn, 
            by = "word")

twentyone_afinn <-    
 inner_join(twentyone_words, 
            afinn, 
            by = "word")

twenty_mean_afinn <- 
  twenty_afinn %>% 
  summarise(mean = mean(value))

twentyone_mean_afinn <- 
  twentyone_afinn %>% 
  summarise(mean_tt_afinn = mean(value))

twenty_nested <- 
  twenty_words %>% 
  group_by(id) %>% 
  nest(word = word) %>% 
  mutate(text = paste(word %>% unlist(), collapse = ' ')) %>% 
  ungroup()

twenty_toDfm <- 
  lapply(X = twenty_nested$text, 
         FUN = function(t) gsub(pattern = "'", 
                                replacement = "",
                                x = t)) %>% 
  unlist()

twenty_dfm <- 
  dfm(twenty_toDfm, 
      remove = c(stopwords("english")),
      stem = T, 
      removePunct = T, 
      removeSeparators = T, 
      removeNumbers = T, 
      removeSymbols = T,
      verbose = T)

set.seed(1)
tmod_twenty <- textmodel_lda(twenty_dfm, k = 10)

# assign topic as a new document-level variable
twenty_dfm$topic <- topics(tmod_twenty)

twentyone_nested <- 
  twentyone_words %>% 
  group_by(id) %>% 
  nest(word = word) %>% 
  mutate(text = paste(word %>% unlist(), collapse = ' ')) %>% 
  ungroup()

twentyone_toDfm <- 
  lapply(X = twentyone_nested$text, 
         FUN = function(t) gsub(pattern = "'", 
                                replacement = "",
                                x = t)) %>% 
  unlist()

twentyone_dfm <- 
  dfm(twentyone_toDfm, 
      remove = c(stopwords("english")),
      stem = T, 
      removePunct = T, 
      removeSeparators = T, 
      removeNumbers = T, 
      removeSymbols = T,
      verbose = T)

set.seed(1)
tmod_twentyone <- textmodel_lda(twentyone_dfm, k = 10)

# assign topic as a new document-level variable
twenty_dfm$topic <- topics(tmod_twenty)
twentyone_dfm$topic <- topics(tmod_twentyone)

toks_tweets <- tokens(twenty_toDfm, remove_punct = TRUE) %>% 
               tokens_keep(pattern = "#*")
dfmat_tweets <- dfm(toks_tweets)

tstat_freq_twenty <- textstat_frequency(dfmat_tweets, n = 30)

toks_tweets <- tokens(twentyone_toDfm, remove_punct = TRUE) %>% 
               tokens_keep(pattern = "#*")
dfmat_tweets <- dfm(toks_tweets)

tstat_freq_twentyone <- textstat_frequency(dfmat_tweets, n = 30)

toks_news20 <- 
  tokens(twenty_toDfm, remove_punct = TRUE) %>% 
  tokens_remove(pattern = c(stopwords("en")), 
                valuetype = "fixed", padding = TRUE)

toks_label20 <- 
  tokens_lookup(toks_news20, 
                dictionary = data_dictionary_newsmap_en, 
                levels = 3) # level 3 is countries

dfmat_label20 <- 
  dfm(toks_label20, tolower = FALSE)

dfmat_feat20 <- 
  dfm(toks_news20, tolower = TRUE)

dfmat_feat_select20 <- 
  dfm_select(dfmat_feat20, 
             pattern = "^[A-Z][A-Za-z0-9]+", 
             valuetype = "regex", 
             case_insensitive = TRUE) %>% 
  dfm_trim(min_termfreq = 10)


toks_news21 <- 
  tokens(twentyone_toDfm, remove_punct = TRUE) %>% 
  tokens_remove(pattern = c(stopwords("en")), 
                valuetype = "fixed", padding = TRUE)

toks_label21 <- 
  tokens_lookup(toks_news21, 
                dictionary = data_dictionary_newsmap_en,
                levels = 3) # level 3 is countries

dfmat_label21 <- 
  dfm(toks_label21, tolower = FALSE)

dfmat_feat21 <- 
  dfm(toks_news21, tolower = TRUE)

dfmat_feat_select21 <- 
  dfm_select(dfmat_feat21, 
             pattern = "^[A-Z][A-Za-z0-9]+", 
             valuetype = "regex", 
             case_insensitive = TRUE) %>% 
  dfm_trim(min_termfreq = 10)

newsmap20 <- textmodel_newsmap(dfmat_feat_select20, y = dfmat_label20)
pred_nm20 <- predict(newsmap20)
count20 <- sort(table(factor(pred_nm20, 
                             levels = colnames(dfmat_label20))), 
                decreasing = TRUE)

newsmap21 <- textmodel_newsmap(dfmat_feat_select21, y = dfmat_label21)
pred_nm21 <- predict(newsmap21)
count21 <- sort(table(factor(pred_nm21, 
                             levels = colnames(dfmat_label21))), 
                decreasing = TRUE)

dat_country20 <- as.data.frame(count20, stringsAsFactors = FALSE)
colnames(dat_country20) <- c("id", "frequency")

dat_country21 <- as.data.frame(count21, stringsAsFactors = FALSE)
colnames(dat_country21) <- c("id", "frequency")

world_map <- map_data(map = "world")
world_map$region <- iso.alpha(world_map$region) # convert country name to ISO code


graph7 <-
  ggplotly(
  ggplot(dat_country20, aes(map_id = id)) +
  geom_map(aes(fill = frequency), map = world_map) +
  expand_limits(x = world_map$long, y = world_map$lat) +
  scale_fill_continuous(name = "Frequency") +
  theme_void() +
  coord_fixed() + 
  ggtitle("30 Mar 2020: Number of Country Mentions on Twitter")
  )

graph8 <-
  ggplotly(
  ggplot(dat_country21, aes(map_id = id)) +
  geom_map(aes(fill = frequency), map = world_map) +
  expand_limits(x = world_map$long, y = world_map$lat) +
  scale_fill_continuous(name = "Frequency") +
  theme_void() +
  coord_fixed() + 
  ggtitle("30 Mar 2021: Number of Country Mentions on Twitter")
  )

twenty_rts <-
  read_csv('2020_retweets.csv') %>% 
  select(-1)

twentyone_rts <-
  read_csv('2021_retweets.csv') %>% 
  select(-1)

twenty_uqtweets <- 
  twenty_rts %>% 
  arrange(desc(Followers)) %>% 
  count(ScreenName) %>% 
  arrange(desc(n))


twenty_rts_toDfm <- 
  lapply(X = twenty_rts$Text, 
         FUN = function(t) gsub(pattern = "'", 
                                replacement = "",
                                x = t)) %>% 
  unlist()

twenty_rts_dfm <- 
  dfm(twenty_rts_toDfm, 
      remove = c(stopwords("english")),
      stem = T, 
      removePunct = T, 
      removeSeparators = T, 
      removeNumbers = T, 
      removeSymbols = T,
      verbose = T)

twenty_rts_dfm$topic <- 
  predict(
    tmod_twenty,
    newdata = twenty_rts_dfm,
    max_iter = 2000,
    verbose = quanteda_options("verbose")
    )

twentyone_rts_toDfm <- 
  lapply(X = twentyone_rts$Text, 
         FUN = function(t) gsub(pattern = "'", 
                                replacement = "",
                                x = t)) %>% 
  unlist()

twentyone_rts_dfm <- 
  dfm(twentyone_rts_toDfm, 
      remove = c(stopwords("english")),
      stem = T, 
      removePunct = T, 
      removeSeparators = T, 
      removeNumbers = T, 
      removeSymbols = T,
      verbose = T)

twentyone_rts_dfm$topic <- 
  predict(
    tmod_twentyone,
    newdata = twentyone_rts_dfm,
    max_iter = 2000,
    verbose = quanteda_options("verbose")
    )

twenty_terms <- terms(tmod_twenty, 20) %>% as.data.frame()
twenty_termslist <- c()
for (x in 1:10) {
  twenty_termslist <- 
    append(twenty_termslist, twenty_terms %>% select(x) %>% 
             unname() %>% unlist())
}
twenty_termslist <- twenty_termslist %>% unique()

#Getting topic-related terms for 2021
twentyone_terms <- terms(tmod_twentyone, 20) %>% as.data.frame()
twentyone_termslist <- c()
for (x in 1:10) {
  twentyone_termslist <- 
    append(twentyone_termslist, twentyone_terms %>% select(x) %>% 
             unname() %>% unlist())
}
twentyone_termslist <- twentyone_termslist %>% unique()

twenty_rts_words <- 
  twenty_rts %>%
  filter(!str_detect(Text, '^"')) %>%
  mutate(text = str_replace_all(Text, 
                                "https://t.co/[A-Za-z\\d]+|&amp;", 
                                "")) %>%
  unnest_tokens(word, text, 
                token = "regex", 
                pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]")) %>% 
  mutate(word = wordStem(word))

twentyone_rts_words <- 
  twentyone_rts %>%
  filter(!str_detect(Text, '^"')) %>%
  mutate(text = str_replace_all(Text, 
                                "https://t.co/[A-Za-z\\d]+|&amp;", 
                                "")) %>%
  unnest_tokens(word, text, 
                token = "regex", 
                pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]")) %>% 
  mutate(word = wordStem(word))

twenty_rts_word_predict <- 
  twenty_rts_words %>% 
  filter(word %in% twenty_termslist) %>% 
  group_by(Text) %>% 
  count(word) %>% 
  ungroup() %>% 
  pivot_wider(id_cols = Text, 
              names_from = word, 
              values_from = n,
              values_fill = 0)

twentyone_rts_word_predict <- 
  twentyone_rts_words %>% 
  filter(word %in% twentyone_termslist) %>% 
  group_by(Text) %>% 
  count(word) %>% 
  ungroup() %>% 
  pivot_wider(id_cols = Text, 
              names_from = word, 
              values_from = n,
              values_fill = 0)

twenty_rts_wordcounts <- 
  twenty_rts %>%
  mutate(tweetLength = str_length(Text)) %>% 
  filter(tweetLength < 500)

twentyone_rts_wordcounts <- 
  twentyone_rts %>%
  mutate(tweetLength = str_length(Text)) %>% 
  filter(tweetLength < 500)

twenty_rts_sentiment <-
  inner_join(twenty_rts_words, nrc, by = "word")

twentyone_rts_sentiment <-
  inner_join(twentyone_rts_words, nrc, by = "word")

twenty_rts_sentiment_analysis <- 
  twenty_rts_sentiment %>% 
  count(word, sentiment) %>% 
  group_by(sentiment)

graph9 <-
  ggplotly(
  twenty_rts_sentiment_analysis %>%  
  top_n(15) %>% 
  ggplot(aes(x = sentiment, y = n )) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ylab("Frequency") +
  xlab("Sentiment") +
  labs(title="30 Mar 2020 Sentiments")
  )

twentyone_rts_sentiment_analysis <- 
  twentyone_rts_sentiment %>% 
  count(word, sentiment) %>% 
  group_by(sentiment)

graph10 <-
  ggplotly(
  twentyone_rts_sentiment_analysis %>%  
  top_n(15) %>% 
  ggplot(aes(x = sentiment, y = n )) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ylab("Frequency") +
  xlab("Sentiment") +
  labs(title="30 Mar 2021 Sentiments")
  )

twentyone_rts_sentiment_analysis %>% filter(!sentiment %in% c("positive", "negative")) %>% 
  mutate(sentiment = reorder(sentiment, -n),
         word = reorder(word, -n)) %>% top_n(10) -> twenty_rts_sentiment_analysis2

graph11 <-
  ggplotly(
  ggplot(twenty_rts_sentiment_analysis2, aes(x=word, y=n, fill = n)) +
  facet_wrap(~ sentiment, scales = "free")+ 
  geom_bar(stat ="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  labs(y="count", title="30 Mar 2020 Sentiment Words")
  )


twentyone_rts_sentiment_analysis %>% filter(!sentiment %in% c("positive", "negative")) %>% 
  mutate(sentiment = reorder(sentiment, -n),
         word = reorder(word, -n)) %>% top_n(10) -> twentyone_rts_sentiment_analysis2

graph12 <-
  ggplotly(
  ggplot(twentyone_rts_sentiment_analysis2, aes(x=word, y=n, fill = n)) +
  facet_wrap(~ sentiment, scales = "free")+ 
  geom_bar(stat ="identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  labs(y="count", title="30 Mar 2021 Sentiment Words")
  )

twenty_rts_afinn <-    
 inner_join(twenty_rts_words, 
            afinn, 
            by = "word")

twentyone_rts_afinn <-    
 inner_join(twentyone_rts_words, 
            afinn, 
            by = "word")

twenty_rts_mean_afinn <- 
  twenty_rts_afinn %>% 
  summarise(mean = mean(value))

twentyone_rts_mean_afinn <- 
  twentyone_afinn %>% 
  summarise(mean = mean(value))

twenty_rtweet_afinn <- 
  twenty_rts_afinn %>% 
  group_by(Text) %>% 
  summarize(afinn = mean(value))

twenty_rts_sentiment_counts <- 
  twenty_rts_sentiment %>% 
  group_by(Text) %>% 
  count(sentiment) %>% 
  ungroup() %>% 
  pivot_wider(id_cols = Text, 
              names_from = sentiment, 
              values_from = n,
              values_fill = 0)

twenty_rts_feature_selection <- 
  twenty_rts_wordcounts %>% 
  left_join(twenty_rts_sentiment_counts, 
            by="Text") %>% 
  left_join(twenty_rtweet_afinn,
            by="Text") %>% 
  left_join(twenty_rts_word_predict,
            by = "Text") %>% 
  mutate(id = row_number()) %>% 
  filter(Followers > 0) %>% 
  mutate(virality = Retweets/Followers)

twentyone_rtweet_afinn <- 
  twentyone_rts_afinn %>% 
  group_by(Text) %>% 
  summarize(afinn = mean(value))

twentyone_rts_sentiment_counts <- 
  twentyone_rts_sentiment %>% 
  group_by(Text) %>% 
  count(sentiment) %>% 
  ungroup() %>% 
  pivot_wider(id_cols = Text, 
              names_from = sentiment, 
              values_from = n,
              values_fill = 0)

twentyone_rts_feature_selection <- 
  twentyone_rts_wordcounts %>% 
  left_join(twentyone_rts_sentiment_counts, 
            by="Text") %>% 
  left_join(twentyone_rtweet_afinn,
            by="Text") %>% 
  left_join(twentyone_rts_word_predict,
            by = "Text") %>% 
  mutate(id = row_number()) %>% 
  filter(Followers > 0) %>% 
  mutate(virality = Retweets/Followers)

for_decisiontree_twenty <-
  twenty_rts_feature_selection %>% select(-1,-2,-3,-4,-5,-6,-`id`) %>% drop_na()
colnames(for_decisiontree_twenty) <- make.names(colnames(for_decisiontree_twenty))

n_features <- length(setdiff(names(for_decisiontree_twenty), "virality"))

rf_model20 <- ranger(
  virality ~ .,
  data = for_decisiontree_twenty,
  mtry = floor(n_features * 0.5),
  respect.unordered.factors = "order",
  importance = "permutation",
  seed = 123
)

for_decisiontree_twentyone <-
  twentyone_rts_feature_selection %>% select(-1,-2,-3,-4,-5,-6,-`id`) %>% drop_na()
colnames(for_decisiontree_twentyone) <- make.names(colnames(for_decisiontree_twentyone))

n_features <- length(setdiff(names(for_decisiontree_twentyone), "virality"))

rf_model21 <- ranger(
  virality ~ .,
  data = for_decisiontree_twentyone,
  mtry = floor(n_features * 0.5),
  respect.unordered.factors = "order",
  importance = "permutation",
  seed = 123
)
```

Examining Original Tweets {.storyboard}
=========================================

### What words appear most frequently in tweets from each sample period? 

```{r}
browsable(div(
  style = "display: flex; flex-wrap: wrap; justify-content: center",
  div(prpgraph_twenty_most_common, style = "width: 50%;"),
  tags$br(),
  div(prpgraph_twentyone_most_common, style = "width: 50%;"),
  p("---")
))

browsable(div(
  style = "display: flex; flex-wrap: wrap; justify-content: center",
  div(graph1, style = "width: 50%;"),
  tags$br(),
  div(graph2, style = "width: 50%;"),
  p("---")
))
```

***

It appears that by 2021, the term "coronavirus" has become a fairly uncommon way to refer to the virus, with COVID becoming the reference of choice. It also seems that over the timeframe, messaging about staying home, social distancing, and Trump has died down and become more undifferentiated from the many other COVID-19-related topics being discussed.

### Is there a difference in tweet lengths between the two samples?

```{r}
print1st <-
  paste0("30 Mar 2020 Mean Tweet Length: ", round(mean(twenty_wordcounts$tweetLength),2))

print2nd <-
  paste0("30 Mar 2021 Mean Tweet Length: ", round(mean(twentyone_wordcounts$tweetLength),2))

hist1 <- 
  ggplotly(
  ggplot(twenty_wordcounts, aes(x=tweetLength)) + geom_histogram() + labs(x = "Character Count") + ggtitle("30 Mar 2020 - Histogram of Tweet Lengths")
  )

hist2 <-
  ggplotly(
  ggplot(twentyone_wordcounts, aes(x=tweetLength)) + geom_histogram() + labs(x = "Character Count") + ggtitle("30 Mar 2021 - Histogram of Tweet Lengths")
  )

test <- t.test(x = twenty_wordcounts$tweetLength, 
               y = twentyone_wordcounts$tweetLength, 
               alternative = "less", var.equal = FALSE)

ttest <- paste(capture.output(test), collapse = "<br>")

browsable(div(
  style = "display: flex; flex-wrap: wrap; justify-content: center",
  p(print1st, tags$br(), print2nd, style = "width: 100%; border: dashed; color: black; font-family: sans-serif;"),
  div(hist1, style = "width: 100%;"),
  tags$br(),
  div(hist2, style = "width: 100%;"),
  div(HTML(ttest), style = "width: 100%; border: dashed; color: black; font-family: sans-serif;")
))
```

***

We check whether there is a difference in tweet lengths between the two samples.

Tweets from March 2020 were on average shorter than those from 2021. It seems that there is a greater share of longer tweets (~300 characters) in 2021.

The difference in mean tweet length is statistically significant, as p-value obtained for the one-sided unpaired two sample t-test is much less than 0.01. The difference is substantial as well---about sixteen fewer characters.

### What differences do we observe in sentiments between the two periods?

```{r}
browsable(div(
  style = "display: flex; flex-wrap: wrap; justify-content: center",
  div(graph3, style = "width: 100%;"),
  tags$br(),
  div(graph4, style = "width: 100%;"),
  p("----")
))
```


***
We joined the NRC Word-Emotion Association Lexicon to our data, which alloweds us to identify words associated with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive).

We produce visualizations comparing the sentiments being expressed in each sample period.

Compared to our 2020 tweets, the 2021 tweets express less trust, less surprise, more sadness, more positivity, less joy, more fear, less disgust, less anticipation, and less anger.

### How positive vs. negative are the tweets from each year?

```{r}
cat(paste0("Average AFINN scores for all words by date\n",
           "\n30 Mar 2020: ", round(twenty_mean_afinn, 3), 
           "\n30 Mar 2021: ", round(twentyone_mean_afinn, 3))) %>% 
  capture.output() %>% 
  paste(collapse = "<br>") %>%
  HTML() %>% div(style = "font-family: sans-serif; border: dashed;")
```

***

Next, we join the AFINN sentiment lexicon, a list of English terms manually rated for valence with an integer between -5 (negative) and +5 (positive) by Finn Årup Nielsen between 2009 and 2011. We use this lexicon to compute mean positivity scores for all words tweeted in each sample year.

The tweets from 2021 are slightly more positive, but the difference appears negligible.


### What topics/discussions are prevalent in tweets published on 30 Mar 2020?

```{r}
key <- 
  kbl(terms(tmod_twenty, 20) %>% as.data.frame(), booktabs = T) %>% 
  kable_styling()

freq <- 
  kbl(table(twenty_dfm$topic))

browsable(div(
  h2("Topic Key", style = "font-family: sans-serif"),
  div(HTML(key), style = "width: 100%;"),
  tags$br(),
  h2("Frequency", style = "font-family: sans-serif"),
  div(HTML(freq), style = "width: 100%;")
))
```


***

Now, we use the quanteda package's implementation of topic modeling to identify what themes/discussions are prevalent in each year. Underlying this topic modeling implementation is Latent Dirichlet allocation (LDA), a machine learning algorithm that learns clusters of words that tend to occur together (topics). Tweets, therefore, are understood as heterogeneous mixtures of these topics. For each tweet, probabilities are assigned for each topic that the document may or may not include, and we will assume that the topic assigned the highest probability by the algorithm is the focus of the tweet.

While these topics may initially seem to make little sense, there are some patterns we can pick out.

Topic 1 seems to be the informational topic concerning outbreaks, data, hospitalizations, deaths, etc.

Topic 2, it would seem, is focused on the crisis' impact on the nation: businesses, governments, schools, and people.

Topic 3 appears to be focused on Trump. More specifically, it seems to be about media-related topics such as press briefings, live news, etc. Topic 3 was among the most frequently discussed topics, as a lot of attention was focused on the president.

Topic 4 seems to encompass emotionally intense tweets reflecting fear, anger, and hope. It includes multiple curse words along with words like "love," "hope," "kill," and "die."

Topic 5 is puzzling; there is no apparent connection between the WHO, Dr. Tedros, and Lady Gaga. We eventually found out that these words correspond to a topic that was trending on 30 Mar 2020: a phone call between WHO Director Dr. Tedros and Lady Gaga. See here: https://twitter.com/drtedros/status/1244008665251708929?lang=en

Topic 6 encompasses tweets urging social distancing.

Topic 7 is the most distinct, as it clearly focuses on the health care situation: mask and ventilator shortages, risks posed to doctors and nurses, and inadequate testing.

Topic 8 centers on China. If we piece together the words, it seems that some of the tweets likely discuss whether it is apt to blame China (note that one of the keywords is "stop"). Additional terms include "travel," "ban," and "hoax," and "lie," which altogether imply that conversations centered on China are interwoven with virus skepticism.

Topic 9 is clearly a political topic; it seems to be discussing national- and state-level policies. For instance, it includes Governor Kemp of Georgia (GA) and the hashtag #gapol, Governor Ron DeSantis of Florida (FL) and the hashtag #flapol, the South Carolina governor press and SC-related hashtags, and, at a national level, the Senate and representatives.

Topic 10 seems to focus on things people are doing at home while quarantining---watching sports in particular: "#stayhom," "#quarantinelif," "read," "play," "watch," "game," and "day." This was the second most prevalent topic. It seems that many people were tweeting about their day-to-day experience in quarantine.


### What topics/discussions are prevalent in tweets published on 30 Mar 2021?


```{r}
key <- 
  kbl(terms(tmod_twentyone, 20) %>% as.data.frame(), booktabs = T) %>% 
  kable_styling()

freq <- 
  kbl(table(twentyone_dfm$topic)) %>% kable_styling()

browsable(div(
  h2("Topic Key", style = "font-family: sans-serif"),
  div(HTML(key), style = "width: 100%;"),
  tags$br(),
  h2("Frequency", style = "font-family: sans-serif"),
  div(HTML(freq), style = "width: 100%;")
))
```


***

The topics from 2021 are harder to interpret.

Topic 1 mentions the Peter Navarro scandal, wherein he---a Trump advisor---allegedly personally profited from questionable/corrupt COVID-19 vaccine investment decisions. None of the other words capture a distinct theme; there are also mentions of taxes, the POTUS, China blaming, etc.

Topic 2 discusses the handling of children and families with respect to schools, travel, and jobs.

Topic 3 seems to be talking about a sports game featuring Brisbane, which people were presumably tuning into. This inference is based on the following words: "watch," "play," "brisban," "game," and "season."

We cannot make sense of topic 4.

Topic 5 focuses on the vaccine rollout.

Topic 6 seems focused on Deborah Birx's comments right around that time, when she claimed that most COVID-19 deaths could have been prevented by Trump and Fauci.

Topic 7 concerns the border crisis---the surge of illegal migrants coming to the US from Mexico, which possibly raises public health fears.

Topic 8 encourages mask wearing and social distancing.

Topic 9 seems informational---focused partly on the COVID situation in China, though the mention of "lab" makes me think that there are conspiracy theories captured by this topic.

Topic 10 is the emotional topic, with angry curse words and words like "love," "feel," "hope," and "God."

### What are the most common hashtags in each sample period?

```{r}
hashtags20 <-
  kbl(head(tstat_freq_twenty, 20), booktabs = T) %>% 
  kable_styling()

hashtags21 <-
  kbl(head(tstat_freq_twentyone, 20), booktabs = T) %>% 
  kable_styling()

browsable(div(
  h2("2020", style = "font-family: sans-serif"),
  div(HTML(hashtags20), style = "width: 100%;"),
  tags$br(),
  h2("2021", style = "font-family: sans-serif"),
  div(HTML(hashtags21), style = "width: 100%;")
))
```

***

In 2020, the most popular hashtags, of course, are #covid19 and #coronavirus, but after these, the hot topics seem to be political topics (#gapol, #flapol, #scpolitics, #boycotttrumppressconferences) and social distancing messaging (#stayhome, #socialdistancing, #quarantinelife, #flattenthecurve).

In 2021, some of the most popular hashtags are anti-CCP, seemingly driven by the theory that China developed/released the virus intentionally. Dr. Li-Meng Yan is one of the key conveyors of this theory, as you can see on her Twitter page: https://twitter.com/DrLiMengYAN1

### How often is each country mentioned in each sample period?


```{r}
cntrymentions20 <-
  head(count20, 10) %>% as.data.frame() %>% 
  ggplot(aes(x = Var1, y = Freq)) + geom_bar(stat = "identity") + labs(x = "country") +
  ggtitle("30 Mar 2020 Country Mentions")

ggplot(dat_country20, aes(map_id = id)) +
      geom_map(aes(fill = frequency), map = world_map) +
      expand_limits(x = world_map$long, y = world_map$lat) +
      scale_fill_continuous(name = "Frequency") +
      theme_void() +
      coord_fixed() + ggtitle("30 Mar 2020: Number of Country Mentions on Twitter") ->
  cntryplot20

cntrymentions21 <-
  head(count21, 10) %>% as.data.frame() %>% 
  ggplot(aes(x = Var1, y = Freq)) + geom_bar(stat = "identity") + labs(x = "country") +
  ggtitle("30 Mar 2021 Country Mentions")

ggplot(dat_country21, aes(map_id = id)) +
      geom_map(aes(fill = frequency), map = world_map) +
      expand_limits(x = world_map$long, y = world_map$lat) +
      scale_fill_continuous(name = "Frequency") +
      theme_void() +
      coord_fixed() + ggtitle("30 Mar 2021: Number of Country Mentions on Twitter") ->
  cntryplot21

browsable(div(
  h2("2020", style = "font-family: sans-serif"),
  style = "display: flex; flex-wrap: wrap;",
  div(ggplotly(cntrymentions20), tags$br(), style = "width: 100%;"),
  div(HTML(ggplot_image(cntryplot20, height = px(600))), style = "width: 100%; justify-content: center;"),
  tags$br(),
  h2("2021", style = "font-family: sans-serif"),
  div(ggplotly(cntrymentions21), tags$br(), style = "width: 100%;"),
  div(HTML(ggplot_image(cntryplot21, height = px(600))), style = "width: 100%; justify-content: center;")
))
```



***

We examine this question for both March of 2020 and 2021. We use the newsmap model as described on the quanteda package website: https://tutorials.quanteda.io/machine-learning/newsmap/

After formatting the data into country-level document feature matrices, we show the estimated number of mentions for each country.

It appears that for both datasets, the most mentions concern the United States. However, in 2020, a greater share of attention is centered on China than on the next two most mentioned locations (Britain and Canada).

We can visualize this using geographic heatmaps.

As you can see, China is brighter in the 2020 map (as is India and Australia to an extent), which indicates a higher frequency of mentions.

In 2021, the English-speaking Twitter user-base seems to be slightly more focused on its home countries than on China, though China still receives substantial attention.


### What words are associated with each country?

```{r}
us20 <- kbl(as.data.frame(coef(newsmap20)$US)) %>% kable_styling()
cn20 <- kbl(as.data.frame(coef(newsmap20)$CN)) %>% kable_styling()
gb20 <- kbl(as.data.frame(coef(newsmap20)$GB)) %>% kable_styling()
ca20 <- kbl(as.data.frame(coef(newsmap20)$CA)) %>% kable_styling()

us21 <- kbl(as.data.frame(coef(newsmap21)$US)) %>% kable_styling()
cn21 <- kbl(as.data.frame(coef(newsmap21)$CN)) %>% kable_styling()
gb21 <- kbl(as.data.frame(coef(newsmap21)$GB)) %>% kable_styling()
ca21 <- kbl(as.data.frame(coef(newsmap21)$CA)) %>% kable_styling()

browsable(div(
  style = "display: flex; flex-wrap: wrap;",
  h2("2020", tags$br(), style = "width: 100%; font-family: sans-serif;"),
  tags$br(),
  div(HTML(us20), style = "width: 50%; border-collapse: separate; border-spacing: 10px; display:table-cell; padding:5px;"),
  div(HTML(cn20), tags$br(), style = "width: 50%; border-collapse: separate; border-spacing: 10px; display:table-cell; padding:5px;"),
  tags$br(),
  div(HTML(gb20), style = "width: 50%; border-collapse: separate; border-spacing: 10px; display:table-cell; padding:5px;"),
  div(HTML(ca20), tags$br(), style = "width: 50%; border-collapse: separate; border-spacing: 10px; display:table-cell; padding:5px;"),
  tags$br(),
  h2("2021", tags$br(), style = "width: 100%; font-family: sans-serif;"),
  tags$br(),
  div(HTML(us21), style = "width: 50%; border-collapse: separate; border-spacing: 10px; display:table-cell; padding:5px;"),
  div(HTML(cn21), tags$br(), style = "width: 50%; border-collapse: separate; border-spacing: 10px; display:table-cell; padding:5px;"),
  tags$br(),
  div(HTML(gb21), style = "width: 50%; border-collapse: separate; border-spacing: 10px; display:table-cell; padding:5px;"),
  div(HTML(ca21), tags$br(), style = "width: 50%; border-collapse: separate; border-spacing: 10px; display:table-cell; padding:5px;")
))
```

***

For each of the two sample periods, we would like to look at what words are associated with each country. We specifically look at the four most mentioned countries in each dataset: the US, China, Great Britain, and Canada.

**2020 Interpretation**

It is difficult to make sense of these words, but there are a few whose meanings are obvious. There seems to be a lot of focus on expertism in the US with words like "scientists," "propaganda," and "expert." There's also discussion of relief bills and certain people being irresponsible.

The China conversation centers around communism, racism, and international travel---to Europe and India.

The Britain-/Canada-related words don't show any obvious themes, but it seems that the ventilator shortage in the UK was one salient topic.

**2021 Interpretation**

It is clear that the conversations surrounding these countries has changed in the past year. Rochelle Walensky, the new CDC Director is one term that sticks out. Others are the discussion of the country reopening, energy policy, and borders. We see that all of these terms are more specific than the general focus on scientists and experts that we saw in 2020. Perhaps our national fog is clearing as our country disseminates the vaccine and progress is made.

The China discussion still centers on theories about the origin of COVID-19, with mentions of a lab, bats, and Wuhan. Racism is still a common topic, especially given the recent hate crimes in the US.

THe UK mentions occur in the contexts of relations with the EU, worries about a new strain, and the Johnson & Johnson vaccine. Concerning Canada, we see talks of the AstraZeneca vaccine, which recently rolled out there. The other words listed are less easy to interpret.


Analyzing Retweets {.storyboard}
=========================================

### Which are the most followed accounts being retweeted in our sample?

```{r}
library(packcircles)

mostfollowers20 <- 
  twenty_rts %>% 
  group_by(ScreenName) %>% 
  summarize(Followers = mean(Followers)) %>% 
  arrange(desc(Followers)) %>%
  head(20) %>% as.data.frame()
packing <- circleProgressiveLayout(mostfollowers20$Followers, sizetype='area')
mostfollowers20 <- cbind(mostfollowers20, packing)
dat.gg <- circleLayoutVertices(packing, npoints=50)

ggplot() + 
  # Make the bubbles
  geom_polygon(data = dat.gg, aes(x, y, group = id, fill=as.factor(id)), colour = "black", alpha = 0.6) +
  # Add text in the center of each bubble + control its size
  geom_text(data = mostfollowers20, aes(x, y, size=Followers, label = ScreenName)) +
  scale_size_continuous(range = c(1,4)) +
  # General theme:
  theme_void() + 
  theme(legend.position="none") +
  coord_equal() -> mostfollowers20

mostfollowers21 <- 
  twenty_rts %>% 
  group_by(ScreenName) %>% 
  summarize(Followers = mean(Followers)) %>% 
  arrange(desc(Followers)) %>%
  head(20) %>% as.data.frame()
packing <- circleProgressiveLayout(mostfollowers21$Followers, sizetype='area')
mostfollowers21 <- cbind(mostfollowers21, packing)
dat.gg <- circleLayoutVertices(packing, npoints=50)

ggplot() + 
  # Make the bubbles
  geom_polygon(data = dat.gg, aes(x, y, group = id, fill=as.factor(id)), colour = "black", alpha = 0.6) +
  # Add text in the center of each bubble + control its size
  geom_text(data = mostfollowers21, aes(x, y, size=Followers, label = ScreenName)) +
  scale_size_continuous(range = c(1,4)) +
  # General theme:
  theme_void() + 
  theme(legend.position="none") +
  coord_equal() -> mostfollowers21

browsable(div(
  style = "display: flex; flex-wrap: wrap;",
  h2("2020 Most Followers in Sample", style = "width: 100%; font-family: sans-serif;"),
  div(ggplotly(mostfollowers20), tags$br(), style = "width: 100%;"),
  h2("2021 Most Followers in Sample", style = "width: 100%; font-family: sans-serif;"),
  div(ggplotly(mostfollowers21), style = "width: 100%;"),
  p("---")
))
```

***

**2020**

Obama was the most followed person getting retweeted on that day, and it seems that Katy Perry was second. India PM Narendra Modi also was getting retweeted at the time, along with several nwes outlets, politicians, and celebrities.

**2021**

In 2021, it seems that Obama was far and away the most followed person getting retweeted with nobody else coming near. News outlets encompass a greater share of the top 20 in follows---perhaps because there are less celebrities talking about COVID-19 in March of 2021 than in 2020.

### Whose tweets aggregately reached the highest retweet counts in each of the two sample periods?

```{r}
twenty_rts %>% 
  group_by(ScreenName) %>% 
  summarize(rt_total = sum(Retweets)) %>% 
  arrange(desc(rt_total)) %>% 
  head(20) %>% kbl(booktabs = T) %>% kable_styling() -> mostrts20

twentyone_rts %>% 
  group_by(ScreenName) %>% 
  summarize(rt_total = sum(Retweets)) %>% 
  arrange(desc(rt_total)) %>% 
  head(20) %>% kbl(booktabs = T) %>% kable_styling() -> mostrts21

browsable(div(
  style = "display: flex; flex-wrap: wrap;",
  h2("2020", style = "width: 100%; font-family: sans-serif;"),
  div(HTML(mostrts20), tags$br(), style = "width: 100%;"),
  h2("2021", style = "width: 100%; font-family: sans-serif;"),
  div(HTML(mostrts21), style = "width: 100%;")
))
```

***

For both lists, most of these names are not particularly recognizable with the exception of Joe Biden, Barack Obama, and Nicolas Maduro (2021).


### Who is the best at getting retweeted (who gets retweeted by the greatest share of followers for each tweet)?

```{r}
twenty_uqtweets <- 
  twenty_rts %>% 
  arrange(desc(Followers)) %>% 
  count(ScreenName) %>% 
  arrange(desc(n))

twenty_rts %>% 
  group_by(ScreenName) %>%
  summarize(Followers = mean(Followers), Retweets = sum(Retweets)) %>% 
  left_join(twenty_uqtweets %>% 
              filter(n>1), by = c('ScreenName')) %>%
  mutate(rt_index = (Retweets/n)/Followers) %>% 
  arrange(desc(rt_index)) %>% 
  head(10) %>% 
  kbl(booktabs = T) %>% kable_styling() -> tweetgame20

twentyone_uqtweets <- 
  twentyone_rts %>% 
  arrange(desc(Followers)) %>% 
  count(ScreenName) %>% 
  arrange(desc(n))

twentyone_rts %>% 
  group_by(ScreenName) %>%
  summarize(Followers = mean(Followers), Retweets = sum(Retweets)) %>% 
  left_join(twentyone_uqtweets %>% 
              filter(n>1), by = c('ScreenName')) %>%
  mutate(rt_index = (Retweets/n)/Followers) %>% 
  arrange(desc(rt_index)) %>% 
  head(10) %>% 
  kbl(booktabs = T) %>% kable_styling() -> tweetgame21

writeLines(c(paste0("30 Mar 2020 Mean RTs: ", 
                  round(mean(twenty_rts$Retweets), 2)), 
           paste0("30 Mar 2021 Mean RTs: ", 
                  round(mean(twentyone_rts$Retweets), 2)))) %>% 
  capture.output() %>% 
  paste(collapse = "<br>") %>% 
  HTML() -> mean_diff_rts

t.test(x = twenty_rts$Retweets, 
       y = twentyone_rts$Retweets, 
       alternative = "greater", var.equal = FALSE) %>% 
  capture.output() %>% paste(collapse = "<br>") %>% HTML() ->
  rt_difftest

browsable(div(
  style = "display: flex; flex-wrap: wrap;",
  h2("2020", style = "width: 100%; font-family: sans-serif;"),
  div(HTML(tweetgame20), tags$br(), style = "width: 100%;"),
  h2("2021", style = "width: 100%; font-family: sans-serif;"),
  div(HTML(tweetgame21), style = "width: 100%;"),
  tags$br(),
  p(mean_diff_rts, tags$br(), rt_difftest, style = "width: 100%; font-family: sans-serif; border: dashed;")
))
```


***

We compute an index for this---a proxy for "Twitter game." First, we determine the number of unique tweets for each screen name. Then, we produce a ratio of number of retweets per tweet to number of followers.

We restricting this analysis to people who have multiple original tweets getting retweeted in our samples. Perhaps this would eliminate from our top ten those users who had one tweet take off but received no attention on their others.

In 2020, camilacousseau over two tweets averaged over six retweets per follower. Nobody else exceeded 55% of this impressive total.

In 2021, richardajabu had an infinitely high retweet index by managing to get two tweets retweeted three times despite having zero followers. Incredible.

Note, however, that overall we observe a stark difference between the retweet indices in 2020 and 2021. Perhaps there were more tweets going viral in March of 2020 than in March of 2021. We test this by examining whether there's a difference in mean retweets for each sample.

Among the tweets that were retweeted (an important specification), the average number of retweets were *much higher* in 2020 than in 2021. Let's see if this mean difference is statistically significant.

An unpaired two sample t-test leads us to accept the one-sided alternative hypothesis that the mean number of retweets on 30 Mar 2020 was substantially greater than the mean number of retweets on the same date in 2021. It seems that people were hitting the retweet button a lot more in the spring of 2020 than in 2021, which makes sense given that people were stuck at home in lockdown with a lot of novel happenings to discuss. Now, with vaccines being rolled out and coronavirus in decline, there is much less panic, anger, and dramatic attention being paid towards the virus.

### In each year, who was the worst at getting retweeted?

```{r}
twenty_rts %>% group_by(ScreenName) %>%
  summarize(Followers = mean(Followers), Retweets = sum(Retweets)) %>% 
  left_join(twenty_uqtweets %>% 
              filter(n>1), by = c('ScreenName')) %>% 
  mutate(rt_index = (Retweets/n)/Followers) %>% arrange(rt_index) %>% head(10) %>% 
  kbl(booktabs = T) %>% kable_styling() -> worstrt20

twentyone_rts %>% group_by(ScreenName) %>%
  summarize(Followers = mean(Followers), Retweets = sum(Retweets)) %>% 
  left_join(twenty_uqtweets %>% 
              filter(n>1), by = c('ScreenName')) %>% 
  mutate(rt_index = (Retweets/n)/Followers) %>% arrange(rt_index) %>% head(10) %>% 
  kbl(booktabs = T) %>% kable_styling() -> worstrt21

browsable(div(
  style = "display: flex; flex-wrap: wrap;",
  h2("2020", style = "width: 100%; font-family: sans-serif;"),
  div(HTML(worstrt20), tags$br(), style = "width: 100%;"),
  h2("2021", style = "width: 100%; font-family: sans-serif;"),
  div(HTML(worstrt21), style = "width: 100%;")
))
```

***

**2020**

ElNacionalWeb, NDTV, Reuters, and the Guardian are all terrible at getting retweeted, which makes sense because they likely tweet a lot of boring, matter-of-fact news as opposed to the clickbait-y headlines that sites like Fox or the New York Times tweet.

**2021**

Again, we see mostly news sites failing to get many retweets. There isn't anything too interesting to be said about this.


### Using our previous topic models, which topics were being retweeted about the most in each year?

```{r}
table(twenty_rts_dfm$topic) %>% as.data.frame() %>% 
  ggplot(aes(x = Var1, y = Freq)) + geom_bar(stat = "identity") + labs(x = "Topic Number") +
  ggtitle("30 Mar 2020 Topic Frequencies") ->
  rt_topics20

table(twentyone_rts_dfm$topic) %>% as.data.frame() %>% 
  ggplot(aes(x = Var1, y = Freq)) + geom_bar(stat = "identity") + labs(x = "Topic Number") +
  ggtitle("30 Mar 2021 Topic Frequencies") ->
  rt_topics21

browsable(div(
  style = "display: flex; flex-wrap: wrap;",
  h2("2020", style = "width: 100%; font-family: sans-serif;"),
  div(ggplotly(rt_topics20), tags$br(), style = "width: 100%;"),
  h2("2021", style = "width: 100%; font-family: sans-serif;"),
  div(ggplotly(rt_topics21), style = "width: 100%;"),
  p("---")
))
```

***

**2020**

It seems that topic 9 was being retweeted about the most, which is very interesting because topic 9 had the fewest original tweets out of all topics we identified from the 2020 data.

Recall that topic 9 was about politics. Perhaps people tend to promote the views of media outlets or political influencers whose content they consume but don't have much to personally contribute to these conversations.

It may also be the case that retweeting and publishing original tweets are zero sum behaviors. In other words, when people are more likely to retweet about a particular topic, maybe that makes them less likely to also tweet about it themselves. Maybe others' have summed up their thoughts better than they can convey.

We will see if this pattern is also seen in the 2021 data.

**2021**

This hypothesized explanation is supported by the 2021 data; topic 1 is the most retweeted about but is the least originally tweeted about. Recall that topic 1 was a sort of catch-all alarmist topic, though it mentions Peter Navarro. It may be the case that retweets displace tweets about the same topic.


### Is there a difference in tweet length among retweets in each sample year?

```{r}
writeLines(c(paste0("30 Mar 2020 Mean Retweeted Tweet Length: ", 
                  mean(twenty_rts_wordcounts$tweetLength)), 
           paste0("30 Mar 2021 Mean Retweeted Tweet Length: ", 
                  mean(twentyone_rts_wordcounts$tweetLength)))) %>% 
  capture.output() %>% 
  paste(collapse = "<br>") %>% 
  HTML() -> rt_lengths

ggplot(data = twenty_rts_wordcounts, aes(x = tweetLength)) + geom_histogram() + ggtitle("30 Mar 2020 - Histogram of Tweet Length") -> rtlength20

ggplot(data = twentyone_rts_wordcounts, aes(x = tweetLength)) + geom_histogram() + ggtitle("30 Mar 2021 - Histogram of Tweet Length") -> rtlength21

browsable(div(
  style = "display: flex; flex-wrap: wrap;",
  p(rt_lengths, tags$br(), style = "width: 100%; font-family: sans-serif; border: dashed"),
  div(ggplotly(rtlength20), tags$br(), style = "width: 100%;"),
  tags$br(),
  div(ggplotly(rtlength21), style = "width: 100%;")
))
```


***

We see that 2020 retweets are slightly shorter and conduct a t-test.

It seems the difference in tweet lengths is statistically significant. 2020 tweets (those being retweeted) were slightly shorter.


### Which sentiments are prevalent among retweeted tweets?

```{r}
twenty_rts_sentiment_analysis %>%  
  top_n(15) %>% 
  ggplot(aes(x = sentiment, y = n )) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ylab("Frequency") +
  xlab("Sentiment") +
  labs(title="30 Mar 2020 Sentiments") -> sentiments_rt_20

twentyone_rts_sentiment_analysis %>%  
  top_n(15) %>% 
  ggplot(aes(x = sentiment, y = n )) +
  geom_bar(stat = "identity") +
  coord_flip() +
  ylab("Frequency") +
  xlab("Sentiment") +
  labs(title="30 Mar 2021 Sentiments") -> sentiments_rt_21

browsable(div(
  style = "display: flex; flex-wrap: wrap;",
  div(ggplotly(sentiments_rt_20), tags$br(), style = "width: 100%;"),
  div(ggplotly(sentiments_rt_21), style = "width: 100%;"),
  p("---")
))
```


***

The sentiments observed are not strikingly different, but there is more sadness, less surprise, more negative sentiments, and less joy being expressed in 2021---over a year from the start of the pandemic in the US.

### Is there a difference between RT AFINN scores in each period?

```{r}
cat(paste0("Average AFINN scores for all retweeted words by date\n",
           "\n30 Mar 2020: ", round(twenty_rts_mean_afinn, 3), 
           "\n30 Mar 2021: ", round(twentyone_rts_mean_afinn, 3))) %>% capture.output() %>% 
  paste(collapse = "<br>") %>% HTML() -> rt_afinns

browsable(div(
  style = "display: flex; flex-wrap: wrap;",
  p(rt_afinns, tags$br(), style = "width: 100%; font-family: sans-serif; border: dashed;")
))
```


***

The 2021 retweets appear to be more negatively valenced.


### What textual features (sentiments and words) predict how viral a tweet became in 2020?

```{r}
vip(rf_model20, num_features = 25) + 
  ggtitle('GBM - 30 Mar 2020 Variable Importance for Virality') -> vip_rf20

library(MASS)
twenty_vars <- importance(rf_model20) %>% as.data.frame() %>% rownames_to_column() %>% arrange(desc(.))
b <- paste(twenty_vars$rowname[1:25], collapse="+")
formula <- as.formula(paste("virality ~ ",b,sep = ""))

# Fit the full model 
full.model <- lm(formula, data = for_decisiontree_twenty %>% drop_na())
# Stepwise regression model
step.model <- stepAIC(full.model, direction = "backward", 
                      trace = FALSE)
tab_model(step.model)$knitr %>% HTML() -> stepmodel20

browsable(div(
  style = "display: flex; flex-wrap: wrap;",
  div(ggplotly(vip_rf20), tags$br(), style = "width: 100%;"),
  p(stepmodel20, style = "width: 100%; font-family: sans-serif; border: dashed;")
))
```


***
Let's see if using sentiments, mean AFINN score, and topic-specific words, we can predict a given tweet's retweet-to-follower ratio (a measure of how viral a tweet was---to what extent it took off). We produce two models for each year---a random forests model and a stepwise regression optimized using AIC. We could include tweet length but opt not to because we are not interested in generating a highly predictive model but understanding what semantic content is being retweeted, and tweet length does not speak strongly to that question.

We are not interested in knowing the predictive efficacy or goodness of fit of the random forests model. We only want to know, out of the features we have selected, which seem the most important to predicting virality. As such, we display variable importance plots for each model and use these for interpretation.

Because the model is predicting both high and low values, the sign of each predictor (positive or negative effect on virality) is unclear. However, we would surmise that features with a strong positive effect would be the most likely to be selected given that most of the variation is positive.

However, because we can't be sure about this, we build a simple backward stepwise linear regression (optimized on AIC) to determine which effects were positive and which were likely negative.

### What textual features (sentiments and words) predict how viral a tweet became in 2021?

```{r}
vip(rf_model21, num_features = 25) + 
  ggtitle('GBM - 30 Mar 2021 Variable Importance for Virality') -> vip_rf21

twentyone_vars <- importance(rf_model21) %>% as.data.frame() %>% rownames_to_column() %>% arrange(desc(.))
b <- paste(twentyone_vars$rowname[1:25], collapse="+")
formula <- as.formula(paste("virality ~ ",b,sep = ""))

# Fit the full model 
full.model <- lm(formula, data = for_decisiontree_twentyone %>% drop_na())
# Stepwise regression model
step.model <- stepAIC(full.model, direction = "backward", 
                      trace = FALSE)
tab_model(step.model)$knitr %>% HTML() -> stepmodel21

browsable(div(
  style = "display: flex; flex-wrap: wrap;",
  div(ggplotly(vip_rf21), tags$br(), style = "width: 100%;"),
  p(stepmodel21, style = "width: 100%; font-family: sans-serif; border: dashed;")
))
```


***

something